{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regresion is used for classification unlike the Liniear Regression which is used to predict numeric values.**"
      ],
      "metadata": {
        "id": "gIgBb5A-_7ZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQtUhGF3-dBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c13057-b16e-4a2a-8fbf-ca8709891ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/SLIIT/Data_Science/Data_Science_Projects/Coursera Projects/TensorFlow/IBM Course of TensorFlow"
      ],
      "metadata": {
        "id": "PWPyxsFB_FbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8ee98c-fe12-4799-8f06-74d2ced3ec62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/SLIIT/Data_Science/Data_Science_Projects/Coursera Projects/TensorFlow/IBM Course of TensorFlow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris #Lets take a dataset from kslearn itself"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IncIG2TV_Hff",
        "outputId": "d68286a4-c10a-4257-9829-4321181eb2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the dataset into iris variable\n",
        "iris = load_iris()\n",
        "\n",
        "#Taking all the records apart from the last raw\n",
        "iris_X, iris_y = iris.data[:-1,:], iris.target[:-1]\n",
        "\n",
        "#applying one-hot encoding on the dependent variable\n",
        "iris_y= pd.get_dummies(iris_y).values\n",
        "\n",
        "#split the dataset into training & testing data\n",
        "trainX, testX, trainY, testY = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "F4-2I1x8FjAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for using one-hot encoding in this scenario, even though the labels are represented by numbers, is to prevent the machine learning model from misinterpreting the numerical labels as having some sort of ordinal or quantitative relationship. Here's a more detailed explanation:\n",
        "\n",
        "Avoiding Implicit Order or Magnitude: When you have categorical data represented as numbers (like 0, 1, 2 for different species), a machine learning model might incorrectly assume that these numbers imply an order (like 2 > 1 > 0) or a magnitude relationship. This is not the case here, as the numbers are purely nominal labels.\n",
        "\n",
        "Treating Each Category Equally: One-hot encoding converts each categorical value into a binary vector. This ensures that each category (species in this case) is treated equally by the model. If we just use 0, 1, 2, the model might infer that the category represented by 2 is somehow 'twice' that of the category represented by 1, which doesn't make sense in this context.\n",
        "\n",
        "Improving Model Performance: Many machine learning models, especially those based on linear assumptions, perform better when categorical data is one-hot encoded. This is because one-hot encoding creates a better feature space that models can leverage to differentiate between the categories.\n",
        "\n",
        "For example, in the Iris dataset:\n",
        "\n",
        "Setosa might be encoded as [1, 0, 0]\n",
        "Versicolour as [0, 1, 0]\n",
        "Virginica as [0, 0, 1]\n",
        "\n",
        "In this way, each species is represented by a distinct vector, and the model can learn separate weights for each species without assuming any numerical relationship between them."
      ],
      "metadata": {
        "id": "cn9-t4xTbJYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numFeatures is the number of features in our input data.\n",
        "# In the iris dataset, this number is '4'.\n",
        "numFeatures = trainX.shape[1]\n",
        "print('numFeatures is : ', numFeatures)\n",
        "# numLabels is the number of classes our data points can be in.\n",
        "# In the iris dataset, this number is '3'.\n",
        "numLabels = trainY.shape[1]\n",
        "print('numLabels is : ', numLabels )\n",
        "\n",
        "#X = tf.Variable( np.identity(numFeatures), tf.TensorShape(numFeatures),dtype='float32') # Iris has 4 features, so X is a tensor to hold our data.\n",
        "#yGold = tf.Variable(np.array([1,1,1]),shape=tf.TensorShape(numLabels),dtype='float32') # This will be our correct answers matrix for 3 classes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcCxm0NSbyCD",
        "outputId": "81e94bc7-95f2-474f-8f6e-c88c1f5a06e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numFeatures is :  4\n",
            "numLabels is :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets convert training and testing sets into TensorFlow objects\n",
        "trainX = tf.constant(trainX, dtype='float32')\n",
        "trainY = tf.constant(trainY, dtype='float32')\n",
        "testX = tf.constant(testX, dtype='float32')\n",
        "testY = tf.constant(testY, dtype='float32')"
      ],
      "metadata": {
        "id": "xAp8V95YmteY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Let's Initialize weights**"
      ],
      "metadata": {
        "id": "naUnpCM0yJZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.zeros([4, 3]))  # 4-dimensional input and  3 classes\n",
        "b = tf.Variable(tf.zeros([3])) # 3-dimensional output [0,0,1],[0,1,0],[1,0,0]"
      ],
      "metadata": {
        "id": "KL7fIwgnm_lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, particularly in a classification task using a model like logistic regression or a basic neural network, the weights (W) and biases (b) play a crucial role. The shapes of W and b are determined based on the structure of the input data and the desired structure of the output data. Let's break down why W and b are shaped this way in your example:\n",
        "\n",
        "  Weights Matrix W:\n",
        "        Shape: The shape of W is [4, 3]. This is because you have a 4-dimensional input and 3 classes to predict. Each feature in the input data must be connected to each class in the output.\n",
        "        Reasoning: In the Iris dataset, there are 4 features (sepal length, sepal width, petal length, petal width). For each class (Setosa, Versicolour, Virginica), the model learns a weight for each feature. Thus, you have 4 features × 3 classes = 12 weight parameters in total.\n",
        "\n",
        "  Biases Vector b:\n",
        "        Shape: The shape of b is [3]. This corresponds to the 3 classes in the output.\n",
        "        Reasoning: The bias allows the model to shift the output function to better fit the data. Each class has its own bias, allowing the model to adjust the output for each class independently.\n",
        "\n",
        "  How They Work Together:\n",
        "        When making a prediction, the input data (a 4-dimensional vector for each instance) is multiplied by the weights matrix W and then the bias b is added. Mathematically, this can be represented as Y = XW + b, where X is your input data.\n",
        "        This operation results in a 3-dimensional vector for each instance, where each dimension corresponds to the model's prediction for each of the three classes. The values are then typically passed through a softmax function to convert them into probabilities.\n",
        "\n",
        "  Example:\n",
        "        Suppose you have a single instance of Iris data: [5.1, 3.5, 1.4, 0.2] (4-dimensional).\n",
        "        This instance is multiplied by W (a 4×3 matrix), resulting in a temporary 3-dimensional vector.\n",
        "        Then, b (a 3-dimensional vector) is added to this temporary vector, resulting in the final 3-dimensional output.\n",
        "\n",
        "In summary, the shape [4, 3] for W and [3] for b directly corresponds to the dimensions of the input data (4 features) and the output predictions (3 possible classes). This setup is crucial for the model to learn the appropriate transformations from inputs to outputs."
      ],
      "metadata": {
        "id": "QZt8ohd1rzfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly sample from a normal distribution with standard deviation .01\n",
        "#This is to initialize the W & b for a neural network\n",
        "weights = tf.Variable(tf.random.normal([numFeatures,numLabels],\n",
        "                                       mean=0.,\n",
        "                                       stddev=0.01,\n",
        "                                       name=\"weights\"),dtype='float32')\n",
        "\n",
        "bias = tf.Variable(tf.random.normal([1,numLabels],\n",
        "                                    mean=0.,\n",
        "                                    stddev=0.01,\n",
        "                                    name=\"bias\"))"
      ],
      "metadata": {
        "id": "LrJAgYW6nTK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now Let's define the hypothesis model**"
      ],
      "metadata": {
        "id": "cfaYY_a7x-6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three-component breakdown of the Logistic Regression equation.\n",
        "# Note that these feed into each other.\n",
        "def logistic_regression(x):\n",
        "    #x is the input & it is first multiplied by weights\n",
        "    apply_weights_OP = tf.matmul(x, weights, name=\"apply_weights\")\n",
        "\n",
        "    #then the result will be added to the biase\n",
        "    add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\")\n",
        "\n",
        "    #then the result will be sent into the sigmoid function\n",
        "    activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
        "\n",
        "    return activation_OP"
      ],
      "metadata": {
        "id": "GlQtd7pSvoMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next, lets define the cost function & other functions**"
      ],
      "metadata": {
        "id": "b696f8TJ1nTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Epochs in our training\n",
        "numEpochs = 700\n",
        "\n",
        "# Defining our learning rate iterations (decay)\n",
        "learningRate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0008,\n",
        "                                          decay_steps=trainX.shape[0],\n",
        "                                          decay_rate= 0.95,\n",
        "                                          staircase=True)\n",
        "\n",
        "#Defining our cost function - Squared Mean Error\n",
        "loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "optimizer = tf.keras.optimizers.SGD(learningRate)\n",
        "\n",
        "# Accuracy metric.\n",
        "def accuracy(y_pred, y_true):\n",
        "# Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "#tf.argmax(y_pred, 1) finds the index of the highest value in each prediction vector. In classification tasks, each element of y_pred is a vector where each value represents the model's confidence that the input corresponds to a particular class.\n",
        "#tf.argmax effectively chooses the class with the highest confidence as the model's prediction.\n",
        "#The 1 in tf.argmax(y_pred, 1) refers to the axis along which to find the argmax. If your prediction is a 2D tensor (batch_size x number_of_classes), axis 1 refers to the number_of_classes axis.\n",
        "\n",
        "#    tf.cast(correct_prediction, tf.float32) converts the boolean tensor to a float tensor (True becomes 1.0 and False becomes 0.0).\n",
        "#    tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) calculates the mean of these values.\n",
        "#    Since the correct predictions are 1.0 and the incorrect ones are 0.0, the mean of this tensor gives the proportion of correct predictions in the batch, which is the accuracy.\n",
        "\n",
        "# Optimization process.\n",
        "def run_optimization(x, y):\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = loss_object(pred, y)\n",
        "    gradients = g.gradient(loss, [weights, bias])\n",
        "    optimizer.apply_gradients(zip(gradients, [weights, bias]))"
      ],
      "metadata": {
        "id": "L0s0NMMf1mq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize reporting variables\n",
        "display_step = 10\n",
        "epoch_values = []\n",
        "accuracy_values = []\n",
        "loss_values = []\n",
        "loss = 0\n",
        "diff = 1\n",
        "\n",
        "# Training epochs\n",
        "for i in range(numEpochs):\n",
        "    #we are minimizing the loss using the optimizer. when its optimized, it will reach a certain point & starts to maximiize again. So we have to identify the right time to stop the optimization.\n",
        "    #So what when the difference of loss become very narrow, we decide that now is the time to stop the optimization. when diff < 0.0001 is that moment.\n",
        "    if i > 1 and diff < .0001:\n",
        "        print(\"change in loss %g; convergence.\"%diff)\n",
        "        break\n",
        "    else:\n",
        "        # Run training step\n",
        "        run_optimization(trainX, trainY)\n",
        "\n",
        "        # Report occasional stats\n",
        "        if i % display_step == 0:\n",
        "            # Add epoch to epoch_values\n",
        "            epoch_values.append(i)\n",
        "\n",
        "            pred = logistic_regression(testX)\n",
        "\n",
        "            newLoss = loss_object(pred, testY)\n",
        "            # Add loss to live graphing variable\n",
        "            loss_values.append(newLoss)\n",
        "\n",
        "            # Generate accuracy stats on test data\n",
        "            acc = accuracy(pred, testY)\n",
        "            accuracy_values.append(acc)\n",
        "\n",
        "\n",
        "            # Re-assign values for variables\n",
        "            diff = abs(newLoss - loss) #abs is a python function that gives absolute value of a number\n",
        "            loss = newLoss\n",
        "\n",
        "            #generate print statements\n",
        "            print(\"step %d, training accuracy %g, loss %g, change in loss %g\"%(i, acc, newLoss, diff))\n",
        "\n",
        "# How well do we perform on held-out test data?\n",
        "print(\"final accuracy on test set: %s\" %acc.numpy())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5GvImqWM5Ed9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Initialization:\n",
        "        display_step, epoch_values, accuracy_values, loss_values are initialized for tracking and reporting purposes.\n",
        "        loss and diff are initialized for tracking the change in loss to check for convergence.\n",
        "\n",
        "  Training Loop:\n",
        "        The loop runs for a number of iterations specified by numEpochs.\n",
        "        It checks if the change in loss (diff) is very small (less than 0.0001) after the first epoch, indicating that the model has potentially converged and the loop can be stopped early.\n",
        "\n",
        "  Training Step:\n",
        "        run_optimization(trainX, trainY) is called to perform a training step. This function updates the model's weights based on the training data.\n",
        "\n",
        "  Reporting and Evaluation:\n",
        "        Every display_step epochs, it performs evaluation and reporting:\n",
        "            epoch_values.append(i) records the current epoch.\n",
        "            pred = logistic_regression(testX) makes predictions on the test dataset.\n",
        "            newLoss = loss_object(pred, testY) computes the loss on the test dataset.\n",
        "            loss_values.append(newLoss) records the computed loss.\n",
        "            acc = accuracy(pred, testY) calculates the accuracy on the test dataset.\n",
        "            accuracy_values.append(acc) records the accuracy.\n",
        "\n",
        "  Loss Difference Calculation:\n",
        "        diff = abs(newLoss - loss) calculates the absolute difference in loss from the previous epoch.\n",
        "        The loss from the current epoch is stored in loss for comparison in the next iteration.\n",
        "\n",
        "  Logging:\n",
        "        The script prints out the current epoch number, training accuracy, loss, and change in loss.\n",
        "\n",
        "  Final Evaluation:\n",
        "        After completing the training epochs, the final accuracy on the test set is printed."
      ],
      "metadata": {
        "id": "tQdcefsV1Dz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mty7H_qU5K4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}